{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S2_Cheese_C1.txt', 'S2_CofHoney_C1.txt', 'S2_Coffee_C1.txt', 'S2_Hotdog_C1.txt', 'S2_Pealate_C1.txt', 'S2_Peanut_C1.txt', 'S2_Tea_C1.txt', 'S3_Cheese_C1.txt', 'S3_CofHoney_C1.txt', 'S3_Coffee_C1.txt', 'S3_Hotdog_C1.txt', 'S3_Pealate_C1.txt', 'S3_Peanut_C1.txt', 'S3_Tea_C1.txt', 'S4_Cheese_C1.txt', 'S4_CofHoney_C1.txt', 'S4_Coffee_C1.txt', 'S4_Hotdog_C1.txt', 'S4_Pealate_C1.txt', 'S4_Peanut_C1.txt', 'S4_Tea_C1.txt']\n"
     ]
    }
   ],
   "source": [
    "vid_list_file = \"/data1/other/EUT/data/gtea/splits/train.split1.bundle\"\n",
    "file_ptr = open(vid_list_file, 'r')\n",
    "list_of_examples = file_ptr.read().split('\\n')[:-1]\n",
    "file_ptr.close()\n",
    "print(list_of_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S1 = '...xxx..x....xxx'\n",
    "B1 = 7\n",
    "S2 = '..xxxxx'\n",
    "B2 = 4\n",
    "S3 = 'xxxxx...x'\n",
    "B3 = 14\n",
    "S4 = '..'\n",
    "B4 = 5\n",
    "S5 = 'xxx'\n",
    "B5 = 4\n",
    "\n",
    "def Solution(S, B):\n",
    "    ans = 0\n",
    "    arr = []\n",
    "    num = 0\n",
    "\n",
    "    for each in S:\n",
    "        if each == 'x':\n",
    "            num += 1\n",
    "        if each == '.' and num:\n",
    "            arr.append(num)\n",
    "            num = 0\n",
    "    if num:\n",
    "        arr.append(num)\n",
    "    arr.sort(reverse=True)\n",
    "\n",
    "    flag = False\n",
    "    for each in arr:\n",
    "        if B - (each+1) >= 0:\n",
    "            ans += each\n",
    "            B -= each+1\n",
    "        else:\n",
    "            flag = True\n",
    "            break\n",
    "\n",
    "    if flag:\n",
    "        ans += B-1\n",
    "    return ans\n",
    "        \n",
    "Solution(S5, B5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a1=[1,1,3]\n",
    "b1=[2,2,1]\n",
    "s1=3\n",
    "a2 = [3,2,3,1]\n",
    "b2 = [1,3,1,2]\n",
    "s2=3\n",
    "a3 = [2,5,6,5]\n",
    "b3 = [5,4,2,2]\n",
    "s3=8\n",
    "\n",
    "a4 = [1,2,1,6,8,7,8]\n",
    "b4 = [2,3,4,7,7,8,7]\n",
    "s4=10\n",
    "\n",
    "def f(A,B,S):\n",
    "    n = len(A)\n",
    "    visited = [False for _ in range(0, S)]\n",
    "\n",
    "    def fff(number):\n",
    "        if number == n:\n",
    "            return True\n",
    "\n",
    "        if visited[A[number]-1] and visited[B[number]-1]:\n",
    "            return False\n",
    "            \n",
    "        if not visited[A[number]-1]:\n",
    "            visited[A[number]-1] = True\n",
    "            flagA = fff(number + 1)\n",
    "            visited[A[number]-1] = False\n",
    "            if flagA:\n",
    "                return True\n",
    "        \n",
    "        if not visited[B[number]-1]:\n",
    "            visited[B[number]-1] = True\n",
    "            flagB = fff(number + 1)\n",
    "            visited[B[number]-1] = False\n",
    "            return flagB\n",
    "\n",
    "        return False\n",
    "\n",
    "    return fff(0)\n",
    "\n",
    "\n",
    "print(f(a1,b1,s1))\n",
    "\n",
    "print(f(a2,b2,s2))\n",
    "\n",
    "print(f(a3,b3,s3))\n",
    "\n",
    "print(f(a4,b4,s4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Solution(S):\n",
    "    red_list = []\n",
    "    for i  in range(len(S)):\n",
    "        if S[i] == 'R'\n",
    "        red_list.append(i)\n",
    "    \n",
    "    if len(red_list) == 0:\n",
    "        return 0\n",
    "    \n",
    "    l = red_list[0]\n",
    "    r = red_list[-1]\n",
    "\n",
    "    if (r-l) * (r-l)/2 > 1000000000:\n",
    "        return -1\n",
    "    \n",
    "    min_sum_step = 1000000000\n",
    "    red_num = len(red_list)\n",
    "    for head in range(l, r-(red_num-1)+1):\n",
    "        sum_tep = 0\n",
    "        for cnt in range(red_num):\n",
    "            goal = head + cnt\n",
    "            now = red_list[cnt]\n",
    "            step = abs(now - goal)\n",
    "            sum_step += step\n",
    "        min_sum_step = min(min_sum_step, sum_step)\n",
    "    if min_sum_step > 1000000000:\n",
    "        return -1\n",
    "    return min_sum_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "flag = [False] * 5\n",
    "print(flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125000250000\n"
     ]
    }
   ],
   "source": [
    "ans = 0\n",
    "for i in range(500000+1):\n",
    "    ans += i\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 862)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vid = list_of_examples[10]\n",
    "features = np.load(\"/data1/other/EUT/data/gtea/features/\" + vid.split('.')[0] + '.npy')\n",
    "print(features.shape)  # (2048, 862) , i.e., (D, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/other/miniconda3/envs/eut/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[8, 7, 2, 1, 6, 3, 7, 1, 6, 7, 2, 7, 7, 5],\n",
      "          [6, 9, 4, 4, 9, 3, 6, 1, 5, 4, 7, 8, 2, 7],\n",
      "          [2, 6, 8, 5, 7, 2, 4, 4, 4, 9, 9, 6, 9, 8],\n",
      "          [5, 9, 3, 7, 1, 6, 4, 1, 9, 7, 8, 3, 5, 4],\n",
      "          [7, 3, 9, 4, 1, 6, 8, 9, 7, 6, 4, 5, 2, 3],\n",
      "          [4, 5, 6, 9, 9, 6, 5, 8, 4, 9, 5, 5, 6, 1],\n",
      "          [6, 6, 3, 2, 6, 7, 6, 7, 8, 7, 5, 1, 5, 7],\n",
      "          [1, 3, 6, 3, 5, 4, 1, 7, 8, 6, 9, 3, 6, 2],\n",
      "          [9, 6, 7, 7, 9, 3, 9, 4, 1, 6, 6, 1, 8, 8],\n",
      "          [8, 4, 9, 2, 2, 2, 4, 7, 6, 5, 8, 6, 9, 6]],\n",
      "\n",
      "         [[7, 6, 2, 6, 6, 7, 6, 5, 9, 8, 6, 6, 5, 9],\n",
      "          [7, 4, 3, 8, 4, 8, 6, 1, 6, 7, 6, 5, 3, 2],\n",
      "          [8, 3, 6, 3, 4, 7, 9, 1, 2, 1, 6, 6, 4, 2],\n",
      "          [8, 5, 8, 4, 5, 2, 1, 5, 1, 9, 5, 4, 6, 9],\n",
      "          [9, 8, 4, 3, 7, 6, 5, 4, 7, 9, 1, 1, 8, 6],\n",
      "          [9, 7, 9, 3, 6, 2, 6, 4, 7, 9, 4, 6, 9, 8],\n",
      "          [9, 6, 9, 3, 7, 7, 1, 1, 1, 1, 5, 3, 1, 4],\n",
      "          [5, 5, 6, 7, 5, 9, 7, 2, 4, 4, 2, 2, 8, 1],\n",
      "          [5, 4, 2, 5, 9, 1, 9, 2, 8, 9, 9, 4, 2, 1],\n",
      "          [5, 1, 4, 9, 9, 6, 9, 5, 4, 6, 7, 6, 2, 6]],\n",
      "\n",
      "         [[5, 9, 6, 8, 5, 2, 8, 6, 1, 7, 5, 7, 2, 2],\n",
      "          [7, 7, 5, 4, 8, 6, 9, 9, 3, 1, 2, 8, 2, 9],\n",
      "          [8, 1, 6, 6, 7, 9, 3, 8, 5, 3, 1, 7, 9, 3],\n",
      "          [7, 3, 6, 3, 5, 5, 5, 3, 7, 2, 5, 1, 3, 2],\n",
      "          [9, 6, 5, 2, 3, 9, 6, 8, 5, 2, 3, 4, 7, 1],\n",
      "          [6, 8, 4, 2, 3, 2, 7, 5, 4, 9, 9, 9, 6, 5],\n",
      "          [6, 6, 6, 7, 9, 9, 7, 8, 9, 8, 6, 7, 9, 4],\n",
      "          [1, 9, 9, 8, 8, 8, 8, 3, 8, 7, 6, 9, 5, 2],\n",
      "          [5, 8, 1, 5, 3, 8, 2, 5, 6, 4, 6, 3, 4, 7],\n",
      "          [8, 9, 7, 3, 3, 4, 5, 1, 3, 4, 2, 7, 5, 7]]],\n",
      "\n",
      "\n",
      "        [[[2, 9, 7, 1, 9, 4, 5, 8, 8, 3, 3, 3, 3, 4],\n",
      "          [1, 2, 3, 4, 7, 3, 2, 1, 1, 6, 8, 5, 5, 6],\n",
      "          [1, 5, 7, 3, 8, 6, 1, 2, 4, 3, 6, 8, 9, 1],\n",
      "          [5, 4, 3, 5, 9, 8, 2, 8, 7, 3, 5, 6, 4, 4],\n",
      "          [3, 2, 9, 9, 5, 7, 6, 2, 1, 4, 5, 9, 5, 4],\n",
      "          [8, 1, 1, 6, 6, 2, 2, 8, 1, 6, 4, 8, 7, 9],\n",
      "          [4, 8, 1, 9, 8, 4, 5, 6, 6, 2, 4, 7, 2, 8],\n",
      "          [5, 2, 8, 5, 1, 1, 9, 6, 9, 3, 2, 2, 8, 6],\n",
      "          [4, 4, 8, 1, 6, 1, 1, 9, 4, 5, 3, 3, 7, 5],\n",
      "          [9, 6, 1, 9, 2, 7, 5, 5, 1, 8, 8, 7, 6, 1]],\n",
      "\n",
      "         [[9, 8, 8, 9, 8, 3, 1, 3, 2, 7, 7, 9, 9, 1],\n",
      "          [2, 4, 5, 3, 8, 9, 6, 5, 1, 4, 1, 6, 6, 6],\n",
      "          [2, 5, 2, 8, 8, 5, 4, 2, 9, 1, 5, 9, 7, 7],\n",
      "          [8, 1, 3, 2, 1, 8, 7, 9, 5, 2, 9, 3, 1, 9],\n",
      "          [7, 4, 3, 8, 4, 8, 3, 1, 1, 2, 3, 3, 9, 6],\n",
      "          [6, 5, 4, 8, 3, 5, 7, 9, 7, 6, 8, 6, 6, 1],\n",
      "          [1, 8, 9, 4, 5, 5, 7, 7, 4, 9, 6, 5, 5, 7],\n",
      "          [7, 4, 5, 5, 9, 4, 5, 1, 5, 4, 9, 3, 9, 9],\n",
      "          [4, 3, 8, 2, 2, 6, 1, 1, 1, 6, 9, 8, 2, 4],\n",
      "          [7, 1, 6, 2, 4, 6, 6, 5, 1, 8, 7, 8, 1, 9]],\n",
      "\n",
      "         [[2, 2, 1, 7, 6, 5, 2, 6, 2, 6, 1, 3, 9, 8],\n",
      "          [2, 2, 6, 5, 7, 6, 9, 2, 5, 7, 2, 8, 3, 8],\n",
      "          [1, 3, 2, 9, 6, 2, 7, 7, 6, 4, 1, 4, 1, 3],\n",
      "          [9, 8, 5, 5, 3, 3, 5, 8, 2, 1, 2, 3, 8, 9],\n",
      "          [7, 5, 1, 3, 5, 5, 7, 5, 1, 3, 9, 2, 5, 2],\n",
      "          [1, 6, 6, 3, 1, 3, 2, 3, 6, 8, 4, 4, 8, 7],\n",
      "          [4, 3, 5, 5, 3, 9, 6, 7, 9, 8, 9, 4, 6, 2],\n",
      "          [8, 9, 2, 4, 6, 5, 6, 3, 8, 6, 1, 8, 5, 9],\n",
      "          [8, 3, 1, 3, 6, 3, 8, 4, 7, 3, 7, 7, 9, 4],\n",
      "          [6, 5, 7, 1, 6, 1, 3, 2, 5, 2, 9, 1, 4, 3]]]])\n",
      "tensor([[[[8, 7, 2, 1, 6],\n",
      "          [9, 4, 4, 9, 3],\n",
      "          [8, 5, 7, 2, 4],\n",
      "          [7, 1, 6, 4, 1],\n",
      "          [1, 6, 8, 9, 7],\n",
      "          [6, 5, 8, 4, 9],\n",
      "          [6, 7, 8, 7, 5],\n",
      "          [7, 8, 6, 9, 3],\n",
      "          [1, 6, 6, 1, 8],\n",
      "          [5, 8, 6, 9, 6]],\n",
      "\n",
      "         [[7, 6, 2, 6, 6],\n",
      "          [4, 3, 8, 4, 8],\n",
      "          [6, 3, 4, 7, 9],\n",
      "          [4, 5, 2, 1, 5],\n",
      "          [7, 6, 5, 4, 7],\n",
      "          [2, 6, 4, 7, 9],\n",
      "          [1, 1, 1, 1, 5],\n",
      "          [2, 4, 4, 2, 2],\n",
      "          [8, 9, 9, 4, 2],\n",
      "          [6, 7, 6, 2, 6]],\n",
      "\n",
      "         [[5, 9, 6, 8, 5],\n",
      "          [7, 5, 4, 8, 6],\n",
      "          [6, 6, 7, 9, 3],\n",
      "          [3, 5, 5, 5, 3],\n",
      "          [3, 9, 6, 8, 5],\n",
      "          [2, 7, 5, 4, 9],\n",
      "          [7, 8, 9, 8, 6],\n",
      "          [3, 8, 7, 6, 9],\n",
      "          [6, 4, 6, 3, 4],\n",
      "          [4, 2, 7, 5, 7]]],\n",
      "\n",
      "\n",
      "        [[[2, 9, 7, 1, 9],\n",
      "          [2, 3, 4, 7, 3],\n",
      "          [7, 3, 8, 6, 1],\n",
      "          [5, 9, 8, 2, 8],\n",
      "          [5, 7, 6, 2, 1],\n",
      "          [2, 2, 8, 1, 6],\n",
      "          [5, 6, 6, 2, 4],\n",
      "          [6, 9, 3, 2, 2],\n",
      "          [4, 5, 3, 3, 7],\n",
      "          [8, 8, 7, 6, 1]],\n",
      "\n",
      "         [[9, 8, 8, 9, 8],\n",
      "          [4, 5, 3, 8, 9],\n",
      "          [2, 8, 8, 5, 4],\n",
      "          [2, 1, 8, 7, 9],\n",
      "          [4, 8, 3, 1, 1],\n",
      "          [5, 7, 9, 7, 6],\n",
      "          [7, 7, 4, 9, 6],\n",
      "          [1, 5, 4, 9, 3],\n",
      "          [1, 6, 9, 8, 2],\n",
      "          [8, 7, 8, 1, 9]],\n",
      "\n",
      "         [[2, 2, 1, 7, 6],\n",
      "          [2, 6, 5, 7, 6],\n",
      "          [2, 9, 6, 2, 7],\n",
      "          [5, 3, 3, 5, 8],\n",
      "          [5, 5, 7, 5, 1],\n",
      "          [3, 2, 3, 6, 8],\n",
      "          [6, 7, 9, 8, 9],\n",
      "          [3, 8, 6, 1, 8],\n",
      "          [7, 3, 7, 7, 9],\n",
      "          [2, 9, 1, 4, 3]]]])\n"
     ]
    }
   ],
   "source": [
    "B=2\n",
    "H=3\n",
    "l_q=10\n",
    "window_size = 5\n",
    "l_k= l_q + 2*(window_size//2)\n",
    "x = torch.randint(1,10,(2, 3, l_q, l_k))\n",
    "print(x)\n",
    "\n",
    "y = torch.as_strided(x, (2, 3, l_q, window_size), (H*l_k*l_q, l_k*l_q, l_k+1 ,1))\n",
    "print(y)\n",
    "# torch.diagonal(x, offset=-1, dim1=-2, dim2=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[8, 4, 4, 9, 6],\n",
      "          [5, 3, 2, 7, 6],\n",
      "          [2, 3, 6, 1, 9],\n",
      "          [8, 7, 5, 8, 9],\n",
      "          [4, 5, 9, 4, 6]],\n",
      "\n",
      "         [[9, 3, 3, 3, 9],\n",
      "          [8, 4, 6, 9, 9],\n",
      "          [2, 2, 6, 7, 8],\n",
      "          [6, 9, 2, 6, 4],\n",
      "          [4, 3, 2, 5, 1]],\n",
      "\n",
      "         [[3, 6, 6, 7, 3],\n",
      "          [6, 6, 5, 7, 8],\n",
      "          [6, 6, 9, 9, 5],\n",
      "          [4, 1, 1, 8, 5],\n",
      "          [5, 2, 1, 8, 3]]],\n",
      "\n",
      "\n",
      "        [[[2, 9, 1, 6, 2],\n",
      "          [4, 1, 4, 1, 1],\n",
      "          [9, 4, 4, 2, 1],\n",
      "          [5, 1, 7, 6, 3],\n",
      "          [8, 7, 4, 2, 1]],\n",
      "\n",
      "         [[9, 2, 2, 1, 7],\n",
      "          [1, 7, 3, 5, 4],\n",
      "          [3, 8, 3, 7, 9],\n",
      "          [2, 4, 9, 6, 9],\n",
      "          [7, 9, 6, 9, 5]],\n",
      "\n",
      "         [[5, 5, 8, 3, 6],\n",
      "          [1, 8, 2, 8, 4],\n",
      "          [8, 6, 7, 5, 4],\n",
      "          [3, 2, 9, 3, 6],\n",
      "          [6, 2, 5, 7, 3]]]])\n",
      "tensor([[[[0., 8., 4.],\n",
      "          [5., 3., 2.],\n",
      "          [3., 6., 1.],\n",
      "          [5., 8., 9.],\n",
      "          [4., 6., 0.]],\n",
      "\n",
      "         [[0., 9., 3.],\n",
      "          [8., 4., 6.],\n",
      "          [2., 6., 7.],\n",
      "          [2., 6., 4.],\n",
      "          [5., 1., 0.]],\n",
      "\n",
      "         [[0., 3., 6.],\n",
      "          [6., 6., 5.],\n",
      "          [6., 9., 9.],\n",
      "          [1., 8., 5.],\n",
      "          [8., 3., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 2., 9.],\n",
      "          [4., 1., 4.],\n",
      "          [4., 4., 2.],\n",
      "          [7., 6., 3.],\n",
      "          [2., 1., 0.]],\n",
      "\n",
      "         [[0., 9., 2.],\n",
      "          [1., 7., 3.],\n",
      "          [8., 3., 7.],\n",
      "          [9., 6., 9.],\n",
      "          [9., 5., 0.]],\n",
      "\n",
      "         [[0., 5., 5.],\n",
      "          [1., 8., 2.],\n",
      "          [6., 7., 5.],\n",
      "          [9., 3., 6.],\n",
      "          [7., 3., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "def extract_dis_from_attention(attention_map, window_size):\n",
    "    \"\"\"\n",
    "    :param attention_map: [B, H, L_seg , (L_seg + 2 * window_size // 2)] or [B, H, L, L]\n",
    "    :return: [B, H, L_seg, window_size]\n",
    "    \"\"\"\n",
    "    B, H, l1, l2 = attention_map.shape\n",
    "    if l1 == l2:\n",
    "        attention_map = torch.cat([torch.zeros(B, H, l1, window_size // 2, device=attention_map.device),\n",
    "                                   attention_map,\n",
    "                                   torch.zeros(B, H, l1, window_size // 2, device=attention_map.device)], dim=-1)\n",
    "        l2 = attention_map.shape[3]\n",
    "\n",
    "    assert attention_map.shape[2] + 2 * (window_size // 2) == attention_map.shape[3]\n",
    "    dis =  torch.as_strided(attention_map, (B, H, l1, window_size), (H*l1*l2, l1*l2, l2+1, 1))\n",
    "    return dis\n",
    "\n",
    "B=2\n",
    "H=3\n",
    "l_q=5\n",
    "window_size = 3\n",
    "l_k= l_q # + 2*(window_size//2)\n",
    "x = torch.randint(1,10,(B, H, l_q, l_k))\n",
    "print(x)\n",
    "print(extract_dis_from_attention(x, window_size=window_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [6, 6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0],\n",
       "        [6, 6, 6, 5, 4, 3, 2, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_q=6\n",
    "l_k=12\n",
    "band_width=7\n",
    "\n",
    "query_position = torch.arange(l_q, dtype=torch.long)[:, None]\n",
    "key_position = torch.arange(l_k, dtype=torch.long)[None, :]\n",
    "\n",
    "relative_position = query_position - key_position\n",
    "rpe = torch.clamp(relative_position, -(band_width // 2), band_width // 2) + band_width // 2\n",
    "\n",
    "rpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[7, 5, 8, 9, 9, 3],\n",
      "         [7, 7, 9, 8, 7, 8],\n",
      "         [5, 9, 6, 3, 8, 9],\n",
      "         [7, 6, 1, 5, 6, 7]],\n",
      "\n",
      "        [[8, 6, 8, 8, 2, 5],\n",
      "         [8, 2, 3, 7, 6, 6],\n",
      "         [1, 6, 6, 5, 5, 2],\n",
      "         [1, 5, 6, 2, 6, 2]]])\n",
      "tensor([[[ 5.6247,  6.3151,  8.2921,  9.0770,  7.2620,  3.4155],\n",
      "         [ 7.7954,  5.3000,  8.5344,  8.6048,  7.7171,  7.8906],\n",
      "         [ 4.7040, 10.6476,  7.4153,  4.5811,  7.7300,  8.7874],\n",
      "         [ 6.3585,  4.8713,  1.8384,  4.9996,  6.4987,  7.1924]],\n",
      "\n",
      "        [[ 6.6247,  7.3151,  8.2921,  8.0770,  0.2620,  5.4155],\n",
      "         [ 8.7954,  0.3000,  2.5344,  7.6048,  6.7171,  5.8906],\n",
      "         [ 0.7040,  7.6476,  7.4153,  6.5811,  4.7300,  1.7874],\n",
      "         [ 0.3585,  3.8713,  6.8384,  1.9996,  6.4987,  2.1924]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[7.0000, 5.8415, 8.9093, 9.1411, 8.2432, 2.0411],\n",
      "         [8.0000, 7.5403, 8.5839, 7.0100, 6.3464, 8.2837],\n",
      "         [5.0000, 9.0100, 6.0200, 3.0300, 8.0400, 9.0500],\n",
      "         [8.0000, 6.9999, 1.9998, 5.9995, 6.9992, 7.9988]],\n",
      "\n",
      "        [[8.0000, 6.8415, 8.9093, 8.1411, 1.2432, 4.0411],\n",
      "         [9.0000, 2.5403, 2.5839, 6.0100, 5.3464, 6.2837],\n",
      "         [1.0000, 6.0100, 6.0200, 5.0300, 5.0400, 2.0500],\n",
      "         [2.0000, 5.9999, 6.9998, 2.9996, 6.9992, 2.9988]]])\n",
      "tensor([[4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [4, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [4, 4, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [4, 4, 4, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [4, 4, 4, 4, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [4, 4, 4, 4, 4, 4, 3, 2, 1, 0, 0, 0, 0, 0],\n",
      "        [4, 4, 4, 4, 4, 4, 4, 3, 2, 1, 0, 0, 0, 0],\n",
      "        [4, 4, 4, 4, 4, 4, 4, 4, 3, 2, 1, 0, 0, 0],\n",
      "        [4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 2, 1, 0, 0],\n",
      "        [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 2, 1, 0]])\n",
      "torch.Size([1, 2, 10, 14])\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoding Examples\n",
    "\n",
    "x = torch.randint(1,10,(2, 4, 6))\n",
    "print(x)\n",
    "\n",
    "\n",
    "# LearnableAPE\n",
    "emb = nn.Embedding(10, 4)\n",
    "position_ids=torch.arange(10)[:6]\n",
    "ape = emb(position_ids).transpose(0, 1)\n",
    "print(x + ape[None, :, :])\n",
    "\n",
    "\n",
    "# SinusoidalAPE\n",
    "pe = torch.zeros(10, 4)\n",
    "position = torch.arange(0, 10, dtype=torch.float).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, 4, 2).float() * (-math.log(10000.0) / 4))\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)\n",
    "ape = pe.unsqueeze(0)\n",
    "print(x + ape[:, :x.size(2), :].transpose(1, 2))\n",
    "\n",
    "\n",
    "# RPE\n",
    "class RPE(nn.Module):\n",
    "    def __init__(self, window_size, h):\n",
    "        super(RPE, self).__init__()\n",
    "        self.h = h\n",
    "        self.window_size = window_size\n",
    "        self.embeddings = nn.Embedding(window_size, h)\n",
    "\n",
    "    def compute_bias(self, l_q, l_k):\n",
    "        \"\"\"\n",
    "        return: [1 h l_q l_k]\n",
    "        \"\"\"\n",
    "        query_position = torch.arange(l_q, dtype=torch.long, device=self.embeddings.weight.device)[:, None]\n",
    "        key_position = torch.arange(l_k, dtype=torch.long, device=self.embeddings.weight.device)[None, :]\n",
    "\n",
    "        relative_position = query_position - key_position\n",
    "        if l_q == l_k:\n",
    "            rpe = torch.clamp(relative_position, -(self.window_size // 2), self.window_size // 2) + self.window_size // 2\n",
    "        elif l_q + 2 * (self.window_size // 2) == l_k:\n",
    "            rpe = torch.clamp(relative_position, -2 * (self.window_size // 2), 0) + 2 * (self.window_size // 2)\n",
    "        else:\n",
    "            raise RuntimeError(\"Wrong RPE!\")\n",
    "\n",
    "        print(rpe)\n",
    "\n",
    "        bias = self.embeddings(rpe)\n",
    "        bias = rearrange(bias, 'm n h -> 1 h m n')\n",
    "\n",
    "        return bias\n",
    "\n",
    "r = RPE(5, 2)\n",
    "a = r.compute_bias(10,14)\n",
    "print(a.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True, False, False, False, False, False,\n",
       "         False],\n",
       "        [False,  True,  True,  True,  True,  True, False, False, False, False,\n",
       "         False],\n",
       "        [False, False,  True,  True,  True,  True,  True, False, False, False,\n",
       "         False],\n",
       "        [False, False, False,  True,  True,  True,  True,  True, False, False,\n",
       "         False],\n",
       "        [False, False, False, False,  True,  True,  True,  True,  True, False,\n",
       "         False],\n",
       "        [False, False, False, False, False,  True,  True,  True,  True,  True,\n",
       "         False],\n",
       "        [False, False, False, False, False, False,  True,  True,  True,  True,\n",
       "          True]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_window_mask(l_seg, band_width):\n",
    "    \"\"\"\n",
    "    :param l_seg: query segment length\n",
    "    :param band_width: an odd, represents window size\n",
    "    :return: a mask matrix with shape of (l_seg, l_seg + 2 * b//2), the positions of elements which participate in calculation are 1\n",
    "    \"\"\"\n",
    "    mask = torch.ones(l_seg, l_seg + 2 * (band_width // 2))\n",
    "    mask = torch.tril(mask, diagonal=band_width-1)\n",
    "    mask = torch.triu(mask, diagonal=0)\n",
    "    return mask.bool()\n",
    "\n",
    "create_window_mask(7,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 5., 7., 1., 1., 5., 1., 4., 4., 8., 5., 2., 0., 0., 0., 0.],\n",
      "         [0., 5., 6., 7., 3., 7., 9., 3., 8., 6., 1., 9., 6., 0., 0., 0., 0.],\n",
      "         [0., 2., 8., 3., 5., 7., 7., 9., 1., 7., 6., 6., 6., 0., 0., 0., 0.],\n",
      "         [0., 5., 6., 3., 8., 3., 7., 6., 2., 1., 1., 6., 5., 0., 0., 0., 0.]]])\n",
      "tensor([[[0., 1., 5., 7., 1., 1., 5.],\n",
      "         [0., 5., 6., 7., 3., 7., 9.],\n",
      "         [0., 2., 8., 3., 5., 7., 7.],\n",
      "         [0., 5., 6., 3., 8., 3., 7.]],\n",
      "\n",
      "        [[1., 5., 1., 4., 4., 8., 5.],\n",
      "         [7., 9., 3., 8., 6., 1., 9.],\n",
      "         [7., 7., 9., 1., 7., 6., 6.],\n",
      "         [3., 7., 6., 2., 1., 1., 6.]],\n",
      "\n",
      "        [[8., 5., 2., 0., 0., 0., 0.],\n",
      "         [1., 9., 6., 0., 0., 0., 0.],\n",
      "         [6., 6., 6., 0., 0., 0., 0.],\n",
      "         [1., 6., 5., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "q=torch.randint(1,10,(1, 4, 12))\n",
    "k=torch.randint(1,10,(1, 4, 12))\n",
    "v=torch.randint(1,10,(1, 4, 12))\n",
    "B, d_model, L = q.size()\n",
    "device = q.device\n",
    "\n",
    "l_seg = 5\n",
    "window_size = 3\n",
    "h = 2\n",
    "\n",
    "n_seg = L // l_seg  # segment num\n",
    "\n",
    "# padding for the last segment\n",
    "if L % l_seg != 0:\n",
    "    q = torch.cat([q, torch.zeros((B, d_model, l_seg - L % l_seg), device=device)], dim=-1)\n",
    "    k = torch.cat([k, torch.zeros((B, d_model, l_seg - L % l_seg), device=device)], dim=-1)\n",
    "    v = torch.cat([v, torch.zeros((B, d_model, l_seg - L % l_seg), device=device)], dim=-1)\n",
    "    n_seg += 1\n",
    "\n",
    "# padding at the start and end of the key and value\n",
    "k = torch.cat([torch.zeros((B, d_model, window_size // 2), device=device), k,\n",
    "                torch.zeros((B, d_model, window_size // 2), device=device)], dim=-1)\n",
    "# v = torch.cat([torch.zeros((B, d_model, window_size // 2), device=device), v,\n",
    "#                 torch.zeros((B, d_model, window_size // 2), device=device)], dim=-1)\n",
    "\n",
    "# print(q)\n",
    "print(k)\n",
    "# print(v)\n",
    "\n",
    "q_seg = rearrange(q, 'b (h d) (n l) -> (b n) h d l', n=n_seg, h=h)  # (B*N, H, D, l_q)\n",
    "# print(q_seg.reshape(B*n_seg, -1, l_seg))\n",
    "\n",
    "k_seg = k.unfold(2, (l_seg + 2 * (window_size // 2)), l_seg)  # (B,D,N,l_k)\n",
    "# print(k_seg)\n",
    "k_seg = rearrange(k_seg, 'b (h d) n l -> (b n) h d l', h=h)  # (B*N,H,D,l_k)\n",
    "print(k_seg.reshape(B*n_seg, d_model, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False, False, False, False, False, False],\n",
       "        [False,  True, False, False, False, False, False, False, False, False],\n",
       "        [False, False,  True, False, False, False, False, False, False, False],\n",
       "        [False, False, False,  True, False, False, False, False, False, False],\n",
       "        [False, False, False, False,  True, False, False, False, False, False],\n",
       "        [False, False, False, False, False,  True, False, False, False, False],\n",
       "        [False, False, False, False, False, False,  True, False, False, False],\n",
       "        [False, False, False, False, False, False, False,  True, False, False],\n",
       "        [False, False, False, False, False, False, False, False,  True, False],\n",
       "        [False, False, False, False, False, False, False, False, False,  True]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_window_mask(l_seg, window_size):\n",
    "    \"\"\"\n",
    "    :param l_seg: query segment length\n",
    "    :param window_size: an odd, represents window size\n",
    "    :return: a mask matrix with shape of (l_seg, l_seg + 2 * b//2), the positions of elements which participate in calculation are 1\n",
    "    \"\"\"\n",
    "    mask = torch.ones(l_seg, l_seg + 2 * (window_size // 2))\n",
    "    mask = torch.tril(mask, diagonal=window_size-1)\n",
    "    mask = torch.triu(mask, diagonal=0)\n",
    "    return mask.bool()\n",
    "\n",
    "create_window_mask(10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True, False, False, False, False]],\n",
      "\n",
      "        [[False,  True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True, False, False, False, False]],\n",
      "\n",
      "        [[False, False, False, False, False, False, False]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False,  True,  True,  True,  True],\n",
       "         [ True, False, False, False,  True,  True,  True],\n",
       "         [ True,  True, False, False, False,  True,  True],\n",
       "         [ True,  True,  True, False, False, False,  True],\n",
       "         [ True,  True,  True,  True, False, False, False]],\n",
       "\n",
       "        [[False, False, False,  True,  True,  True,  True],\n",
       "         [ True, False, False, False,  True,  True,  True],\n",
       "         [ True,  True, False, False, False,  True,  True],\n",
       "         [ True,  True,  True, False, False, False,  True],\n",
       "         [ True,  True,  True,  True, False, False, False]],\n",
       "\n",
       "        [[False, False, False,  True,  True,  True,  True],\n",
       "         [ True, False, False,  True,  True,  True,  True],\n",
       "         [ True,  True, False,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True]],\n",
       "\n",
       "        [[ True, False, False,  True,  True,  True,  True],\n",
       "         [ True, False, False, False,  True,  True,  True],\n",
       "         [ True,  True, False, False, False,  True,  True],\n",
       "         [ True,  True,  True, False, False, False,  True],\n",
       "         [ True,  True,  True,  True, False, False, False]],\n",
       "\n",
       "        [[False, False, False,  True,  True,  True,  True],\n",
       "         [ True, False, False,  True,  True,  True,  True],\n",
       "         [ True,  True, False,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_window_mask(l_seg, window_size):\n",
    "    \"\"\"\n",
    "    :param l_seg: query segment length\n",
    "    :param window_size: an odd, represents window size\n",
    "    :return: a mask matrix with shape of (l_seg, l_seg + 2 * b//2), the positions of elements which participate in calculation are 1\n",
    "    \"\"\"\n",
    "    mask = torch.ones(l_seg, l_seg + 2 * (window_size // 2))\n",
    "    mask = torch.tril(mask, diagonal=window_size-1)\n",
    "    mask = torch.triu(mask, diagonal=0)\n",
    "    return mask.bool()\n",
    "\n",
    "window_mask = create_window_mask(l_seg, window_size)  # (l_q, l_k)\n",
    "# print(window_mask)\n",
    "\n",
    "\n",
    "input_mask = torch.ones(2, 12)\n",
    "B = 2\n",
    "input_mask[1, -5:] = 0\n",
    "# print(input_mask)\n",
    "mask = torch.cat([input_mask, torch.zeros((B, l_seg - L % l_seg), device=device)], dim=-1)\n",
    "mask = torch.cat([torch.zeros((B, window_size // 2), device=device), mask, \n",
    "                    torch.zeros((B, window_size // 2), device=device)], dim=-1)\n",
    "mask = mask.unfold(1, (l_seg + 2 * (window_size // 2)), l_seg)  # (B, N, l_k)\n",
    "mask = mask.reshape(-1, 1, l_seg + 2 * (window_size // 2)).bool()  # (B*N, 1, l_k)\n",
    "print(mask)\n",
    "~ (window_mask & mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2., 2., 3., 1., 2., 2., 4., 1., 3., 2.],\n",
      "         [3., 1., 4., 1., 4., 4., 3., 4., 3., 2.],\n",
      "         [3., 3., 4., 4., 3., 1., 4., 1., 3., 2.],\n",
      "         [1., 2., 4., 3., 2., 4., 1., 4., 3., 1.]],\n",
      "\n",
      "        [[1., 3., 3., 2., 4., 2., 2., 4., 4., 1.],\n",
      "         [1., 4., 4., 2., 1., 1., 1., 3., 1., 1.],\n",
      "         [2., 2., 3., 4., 2., 2., 2., 4., 2., 1.],\n",
      "         [4., 1., 1., 4., 3., 3., 4., 2., 3., 3.]]])\n",
      "tensor([[[3., 2., 1., 2., 3., 3., 1., 2., 2., 1.],\n",
      "         [4., 2., 2., 4., 1., 1., 3., 4., 1., 1.],\n",
      "         [3., 4., 1., 3., 3., 3., 3., 3., 4., 1.],\n",
      "         [1., 3., 4., 1., 3., 1., 2., 4., 4., 3.]],\n",
      "\n",
      "        [[1., 2., 4., 4., 3., 3., 3., 3., 4., 1.],\n",
      "         [1., 4., 1., 2., 2., 4., 2., 2., 4., 1.],\n",
      "         [1., 3., 4., 1., 1., 3., 1., 1., 1., 3.],\n",
      "         [2., 2., 3., 1., 2., 2., 2., 3., 2., 1.]]])\n",
      "tensor([[[[12.7279,  7.0711,  5.6569, 11.3137,  6.3640,  6.3640,  7.7782,\n",
      "           11.3137,  4.9497,  3.5355],\n",
      "          [ 7.0711,  4.2426,  2.8284,  5.6569,  4.9497,  4.9497,  3.5355,\n",
      "            5.6569,  3.5355,  2.1213],\n",
      "          [17.6777,  9.8995,  7.7782, 15.5563,  9.1924,  9.1924, 10.6066,\n",
      "           15.5563,  7.0711,  4.9497],\n",
      "          [ 4.9497,  2.8284,  2.1213,  4.2426,  2.8284,  2.8284,  2.8284,\n",
      "            4.2426,  2.1213,  1.4142],\n",
      "          [15.5563,  8.4853,  7.0711, 14.1421,  7.0711,  7.0711,  9.8995,\n",
      "           14.1421,  5.6569,  4.2426],\n",
      "          [15.5563,  8.4853,  7.0711, 14.1421,  7.0711,  7.0711,  9.8995,\n",
      "           14.1421,  5.6569,  4.2426],\n",
      "          [16.9706,  9.8995,  7.0711, 14.1421, 10.6066, 10.6066,  9.1924,\n",
      "           14.1421,  7.7782,  4.9497],\n",
      "          [13.4350,  7.0711,  6.3640, 12.7279,  4.9497,  4.9497,  9.1924,\n",
      "           12.7279,  4.2426,  3.5355],\n",
      "          [14.8492,  8.4853,  6.3640, 12.7279,  8.4853,  8.4853,  8.4853,\n",
      "           12.7279,  6.3640,  4.2426],\n",
      "          [ 9.8995,  5.6569,  4.2426,  8.4853,  5.6569,  5.6569,  5.6569,\n",
      "            8.4853,  4.2426,  2.8284]],\n",
      "\n",
      "         [[ 7.0711, 10.6066,  4.9497,  7.0711,  8.4853,  7.0711,  7.7782,\n",
      "            9.1924, 11.3137,  4.2426],\n",
      "          [ 7.7782, 12.7279,  7.7782,  7.7782, 10.6066,  7.7782,  9.1924,\n",
      "           12.0208, 14.1421,  6.3640],\n",
      "          [11.3137, 19.7990, 14.1421, 11.3137, 16.9706, 11.3137, 14.1421,\n",
      "           19.7990, 22.6274, 11.3137],\n",
      "          [10.6066, 17.6777, 11.3137, 10.6066, 14.8492, 10.6066, 12.7279,\n",
      "           16.9706, 19.7990,  9.1924],\n",
      "          [ 7.7782, 12.7279,  7.7782,  7.7782, 10.6066,  7.7782,  9.1924,\n",
      "           12.0208, 14.1421,  6.3640],\n",
      "          [ 4.9497, 11.3137, 12.0208,  4.9497, 10.6066,  4.9497,  7.7782,\n",
      "           13.4350, 14.1421,  9.1924],\n",
      "          [ 9.1924, 13.4350,  5.6569,  9.1924, 10.6066,  9.1924,  9.8995,\n",
      "           11.3137, 14.1421,  4.9497],\n",
      "          [ 4.9497, 11.3137, 12.0208,  4.9497, 10.6066,  4.9497,  7.7782,\n",
      "           13.4350, 14.1421,  9.1924],\n",
      "          [ 8.4853, 14.8492, 10.6066,  8.4853, 12.7279,  8.4853, 10.6066,\n",
      "           14.8492, 16.9706,  8.4853],\n",
      "          [ 4.9497,  7.7782,  4.2426,  4.9497,  6.3640,  4.9497,  5.6569,\n",
      "            7.0711,  8.4853,  3.5355]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4142,  4.2426,  3.5355,  4.2426,  3.5355,  4.9497,  3.5355,\n",
      "            3.5355,  5.6569,  1.4142],\n",
      "          [ 4.9497, 15.5563, 11.3137, 14.1421, 12.0208, 17.6777, 12.0208,\n",
      "           12.0208, 19.7990,  4.9497],\n",
      "          [ 4.9497, 15.5563, 11.3137, 14.1421, 12.0208, 17.6777, 12.0208,\n",
      "           12.0208, 19.7990,  4.9497],\n",
      "          [ 2.8284,  8.4853,  7.0711,  8.4853,  7.0711,  9.8995,  7.0711,\n",
      "            7.0711, 11.3137,  2.8284],\n",
      "          [ 3.5355,  8.4853, 12.0208, 12.7279,  9.8995, 11.3137,  9.8995,\n",
      "            9.8995, 14.1421,  3.5355],\n",
      "          [ 2.1213,  5.6569,  6.3640,  7.0711,  5.6569,  7.0711,  5.6569,\n",
      "            5.6569,  8.4853,  2.1213],\n",
      "          [ 2.1213,  5.6569,  6.3640,  7.0711,  5.6569,  7.0711,  5.6569,\n",
      "            5.6569,  8.4853,  2.1213],\n",
      "          [ 4.9497, 14.1421, 13.4350, 15.5563, 12.7279, 16.9706, 12.7279,\n",
      "           12.7279, 19.7990,  4.9497],\n",
      "          [ 3.5355,  8.4853, 12.0208, 12.7279,  9.8995, 11.3137,  9.8995,\n",
      "            9.8995, 14.1421,  3.5355],\n",
      "          [ 1.4142,  4.2426,  3.5355,  4.2426,  3.5355,  4.9497,  3.5355,\n",
      "            3.5355,  5.6569,  1.4142]],\n",
      "\n",
      "         [[ 7.0711,  9.8995, 14.1421,  4.2426,  7.0711,  9.8995,  7.0711,\n",
      "            9.8995,  7.0711,  7.0711],\n",
      "          [ 2.8284,  5.6569,  7.7782,  2.1213,  2.8284,  5.6569,  2.8284,\n",
      "            3.5355,  2.8284,  4.9497],\n",
      "          [ 3.5355,  7.7782, 10.6066,  2.8284,  3.5355,  7.7782,  3.5355,\n",
      "            4.2426,  3.5355,  7.0711],\n",
      "          [ 8.4853, 14.1421, 19.7990,  5.6569,  8.4853, 14.1421,  8.4853,\n",
      "           11.3137,  8.4853, 11.3137],\n",
      "          [ 5.6569,  8.4853, 12.0208,  3.5355,  5.6569,  8.4853,  5.6569,\n",
      "            7.7782,  5.6569,  6.3640],\n",
      "          [ 5.6569,  8.4853, 12.0208,  3.5355,  5.6569,  8.4853,  5.6569,\n",
      "            7.7782,  5.6569,  6.3640],\n",
      "          [ 7.0711,  9.8995, 14.1421,  4.2426,  7.0711,  9.8995,  7.0711,\n",
      "            9.8995,  7.0711,  7.0711],\n",
      "          [ 5.6569, 11.3137, 15.5563,  4.2426,  5.6569, 11.3137,  5.6569,\n",
      "            7.0711,  5.6569,  9.8995],\n",
      "          [ 5.6569,  8.4853, 12.0208,  3.5355,  5.6569,  8.4853,  5.6569,\n",
      "            7.7782,  5.6569,  6.3640],\n",
      "          [ 4.9497,  6.3640,  9.1924,  2.8284,  4.9497,  6.3640,  4.9497,\n",
      "            7.0711,  4.9497,  4.2426]]]])\n",
      "tensor([[[False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True, False, False, False,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True, False, False, False,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False, False, False,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True, False, False, False,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True, False, False, False,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False]],\n",
      "\n",
      "        [[False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True, False, False, False,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True, False, False, False,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True, False, False, False,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True, False, False, False,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True, False, False, False,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False]]])\n",
      "tensor([[[[9.9652e-01, 3.4813e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [9.3155e-01, 5.5060e-02, 1.3386e-02, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 3.4799e-03, 4.1714e-04, 9.9610e-01, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 8.7949e-02, 7.3368e-01, 1.7837e-01,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9830e-01, 8.4789e-04,\n",
      "           8.4789e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2857e-02,\n",
      "           5.2857e-02, 8.9429e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           2.8124e-02, 6.8375e-03, 9.6504e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 2.8312e-02, 9.7149e-01, 2.0060e-04, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 9.9807e-01, 1.7192e-03, 2.0609e-04],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 8.0443e-01, 1.9557e-01]],\n",
      "\n",
      "         [[2.8318e-02, 9.7168e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [6.9862e-03, 9.8603e-01, 6.9862e-03, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 9.9631e-01, 3.4806e-03, 2.0572e-04, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 2.7928e-02, 1.3770e-02, 9.5830e-01,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2857e-02, 8.9429e-01,\n",
      "           5.2857e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.4109e-01,\n",
      "           3.2877e-03, 5.5624e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           8.7949e-02, 1.7837e-01, 7.3368e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 1.1524e-03, 3.2986e-01, 6.6899e-01, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 1.0702e-01, 8.9279e-01, 1.8435e-04],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9296e-01, 7.0353e-03]]],\n",
      "\n",
      "\n",
      "        [[[5.5807e-02, 9.4419e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [2.4401e-05, 9.8581e-01, 1.4166e-02, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 7.9524e-01, 1.1427e-02, 1.9334e-01, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 1.6358e-01, 6.7284e-01, 1.6358e-01,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 7.6792e-01, 4.5388e-02,\n",
      "           1.8669e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6358e-01,\n",
      "           6.7284e-01, 1.6358e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           6.7284e-01, 1.6358e-01, 1.6358e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 8.4789e-04, 8.4789e-04, 9.9830e-01, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 1.4166e-02, 9.8581e-01, 2.4401e-05],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 9.8583e-01, 1.4166e-02]],\n",
      "\n",
      "         [[5.5807e-02, 9.4419e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [6.2870e-03, 1.0637e-01, 8.8734e-01, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 5.5785e-02, 9.4382e-01, 3.9525e-04, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 9.9999e-01, 7.2134e-07, 1.2204e-05,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 6.6453e-03, 5.5436e-02,\n",
      "           9.3792e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2857e-02,\n",
      "           8.9429e-01, 5.2857e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           4.8565e-01, 2.8705e-02, 4.8565e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 1.6358e-01, 6.7284e-01, 1.6358e-01, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 7.3368e-01, 8.7949e-02, 1.7837e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 6.6976e-01, 3.3024e-01]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[3.9965, 3.9449, 3.9965, 4.0000, 4.0000, 2.2114, 3.9863, 3.9434,\n",
       "          3.9994, 3.4133],\n",
       "         [3.9965, 3.9048, 2.9992, 2.6457, 3.0000, 3.8943, 1.1049, 1.0853,\n",
       "          1.0036, 2.8044],\n",
       "         [1.0850, 1.0349, 1.0074, 1.0834, 1.1586, 1.1702, 3.8241, 2.6620,\n",
       "          2.2139, 1.9930],\n",
       "         [3.9434, 3.9651, 3.9891, 2.9304, 2.8414, 2.8822, 2.4674, 2.9977,\n",
       "          2.9998, 2.9930]],\n",
       "\n",
       "        [[2.1116, 2.0142, 2.0114, 2.1636, 2.3734, 3.1821, 3.0185, 1.0000,\n",
       "          1.0000, 1.0000],\n",
       "         [3.8326, 3.9716, 3.5905, 1.8364, 2.1413, 2.8364, 3.3272, 4.0000,\n",
       "          3.9999, 3.9575],\n",
       "         [1.0558, 1.8936, 1.9446, 2.0000, 3.8825, 3.7357, 3.4282, 2.6728,\n",
       "          3.1784, 3.3302],\n",
       "         [4.0000, 3.1127, 3.0562, 3.0000, 1.0199, 1.1586, 2.5431, 3.5093,\n",
       "          3.7362, 1.9907]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_window_mask(l_seg, window_size):\n",
    "    \"\"\"\n",
    "    :param l_seg: query segment length\n",
    "    :param window_size: an odd, represents window size\n",
    "    :return: a mask matrix with shape of (l_seg, l_seg + 2 * b//2), the positions of elements which participate in calculation are 1\n",
    "    \"\"\"\n",
    "    mask = torch.ones(l_seg, l_seg + 2 * (window_size // 2))\n",
    "    mask = torch.tril(mask, diagonal=window_size-1)\n",
    "    mask = torch.triu(mask, diagonal=0)\n",
    "    return mask.bool()\n",
    "\n",
    "\n",
    "def scalar_dot_attn(queries, keys, values, mask=None, rpe=None, return_attn=False):\n",
    "    \"\"\"\n",
    "    :param queries: (B, H, D_q, l_q)\n",
    "    :param keys:    (B, H, D_k, l_k)\n",
    "    :param values:  (B, H, D_v, l_k)\n",
    "    :param mask:    (B, l_q, l_k)\n",
    "    :param rpe:     (1, H, l_q, l_k)\n",
    "    :return:        out: (B, D_out, l_q)  scores: (B, l, window_size)\n",
    "    \"\"\"\n",
    "    B, H, D_q, L_q = queries.shape\n",
    "    _, _, D_k, L_k = keys.shape\n",
    "    _, _, D_v, L_v = values.shape\n",
    "    assert L_k == L_v\n",
    "    assert D_q == D_k\n",
    "\n",
    "    scores = torch.einsum(\"bhdl,bhdn->bhln\", queries, keys) / math.sqrt(D_q)  # (B, H, l_q, l_k)\n",
    "    print(scores)\n",
    "    print(mask)\n",
    "    \n",
    "\n",
    "    if rpe is not None:\n",
    "        scores = scores + rpe  # (B, H, l_q, l_k) + (1, H, l_q, l_k) = (B, H, l_q, l_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask.unsqueeze(1), -1e9)  # Fill elements of the tensor with -inf where mask is True\n",
    "    \n",
    "    \n",
    "    \n",
    "    scores_norm = F.softmax(scores, dim=-1)  # (B, H, l_q, l_k)\n",
    "    print(scores_norm)\n",
    "    V = torch.einsum(\"bhdn,bhln->bhdl\", values, scores_norm)  # (B, H, D, l_q)\n",
    "    V = V.reshape(B, -1, L_q)  # (B, D_out, l_q)\n",
    "\n",
    "    if return_attn:\n",
    "        attn = None\n",
    "    else:\n",
    "        attn = None\n",
    "\n",
    "    # each_seg_socre = []\n",
    "    # if BALoss:\n",
    "    #     for i in range(B):\n",
    "    #         for j in range(H):\n",
    "    #             each_seg_socre.append(extract_dis_from_attention(scores_norm[i, j, ...], window_size))\n",
    "    #     all_seg_scores = torch.stack(each_seg_socre, dim=0)  # (B*H, L, window_size)\n",
    "    # else:\n",
    "    #     all_seg_scores = None\n",
    "\n",
    "    return V, attn\n",
    "\n",
    "\n",
    "\n",
    "q=torch.randint(1,5,(2, 4, 10)).float()\n",
    "print(q)\n",
    "k=torch.randint(1,5,(2, 4, 10)).float()\n",
    "print(k)\n",
    "v=torch.randint(1,5,(2, 4, 10)).float()\n",
    "input_mask = torch.ones(2, 10)\n",
    "\n",
    "B = 2\n",
    "h = 2\n",
    "L = q.size(2)\n",
    "q = q.reshape(B, h, -1, L)\n",
    "k = k.reshape(B, h, -1, L)\n",
    "v = v.reshape(B, h, -1, L)\n",
    "\n",
    "\n",
    "window_size = 3\n",
    "\n",
    "window_mask = create_window_mask(L, window_size)\n",
    "begin_index = window_size // 2\n",
    "window_mask = window_mask[:, begin_index:L+begin_index]  # (L, L)\n",
    "window_mask = window_mask\n",
    "\n",
    "mask = input_mask.bool().unsqueeze(1)  # (B, 1, L)\n",
    "\n",
    "total_mask = ~ (window_mask & mask)  # (B, L, L)\n",
    "\n",
    "out, attn = scalar_dot_attn(q, k, v, mask=total_mask, return_attn=False)\n",
    "out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.,  ..., 97., 98., 99.],\n",
      "         [ 1.,  0.,  1.,  ..., 96., 97., 98.],\n",
      "         [ 2.,  1.,  0.,  ..., 95., 96., 97.],\n",
      "         ...,\n",
      "         [97., 96., 95.,  ...,  0.,  1.,  2.],\n",
      "         [98., 97., 96.,  ...,  1.,  0.,  1.],\n",
      "         [99., 98., 97.,  ...,  2.,  1.,  0.]],\n",
      "\n",
      "        [[ 0.,  1.,  2.,  ..., 97., 98., 99.],\n",
      "         [ 1.,  0.,  1.,  ..., 96., 97., 98.],\n",
      "         [ 2.,  1.,  0.,  ..., 95., 96., 97.],\n",
      "         ...,\n",
      "         [97., 96., 95.,  ...,  0.,  1.,  2.],\n",
      "         [98., 97., 96.,  ...,  1.,  0.,  1.],\n",
      "         [99., 98., 97.,  ...,  2.,  1.,  0.]],\n",
      "\n",
      "        [[ 0.,  1.,  2.,  ..., 97., 98., 99.],\n",
      "         [ 1.,  0.,  1.,  ..., 96., 97., 98.],\n",
      "         [ 2.,  1.,  0.,  ..., 95., 96., 97.],\n",
      "         ...,\n",
      "         [97., 96., 95.,  ...,  0.,  1.,  2.],\n",
      "         [98., 97., 96.,  ...,  1.,  0.,  1.],\n",
      "         [99., 98., 97.,  ...,  2.,  1.,  0.]]])\n",
      "tensor([6.6739, 6.6739, 6.6739])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.random.manual_seed(0)\n",
    "# a = torch.randint(1,10,(2, 5, 1)).float()\n",
    "# b = torch.randint(1,10,(2, 5, 1)).float()\n",
    "from geomloss import SamplesLoss\n",
    "\n",
    "weight_c = torch.softmax(torch.randn(100),dim=-1).unsqueeze(0).repeat(3,1)  # (B, sample_number)\n",
    "c = torch.arange(100).float().unsqueeze(1).unsqueeze(0)\n",
    "weight_d = torch.softmax(torch.randn(100),dim=-1).unsqueeze(0).repeat(3,1)\n",
    "d = torch.arange(100).float().unsqueeze(1).unsqueeze(0)\n",
    "\n",
    "def my_abs(x, y):\n",
    "    # (B, D, 1)\n",
    "    if x.dim() == 3 and y.dim() == 3:\n",
    "        return torch.abs(x.transpose(1, 2)-y)\n",
    "    raise ValueError(\"Input samples 'x' and 'y' should be encoded as (B,N,D) (batch) tensors.\")\n",
    "\n",
    "    \n",
    "print(my_abs(c.repeat(3,1,1),d.repeat(3,1,1)))\n",
    "\n",
    "WLoss = SamplesLoss(\"sinkhorn\", backend='tensorized', cost=my_abs, blur=0.0001, debias=False, scaling=0.99999)\n",
    "\n",
    "print(WLoss(weight_c,c.repeat(3,1,1),weight_d,d.repeat(3,1,1)))\n",
    "\n",
    "# # print(WLoss(a[0,0,:], b[0,0,:]))\n",
    "# # print(WLoss(a[0,...], b[0,...]))\n",
    "# # print(WLoss(a[1,...], b[1,...]))\n",
    "# # print(WLoss(a, b))\n",
    "\n",
    "# # WLoss中的输入只能是2维或3维，如果为3维，则是(B, sample_number, feature_dim)，如果是2维则 (sample_number, feature_dim)，即B=0\n",
    "\n",
    "# def my_kl_loss(p, q):\n",
    "#     res = p * (torch.log(p + 0.0001) - torch.log(q + 0.0001))\n",
    "#     return torch.mean(torch.sum(res, dim=-1), dim=1)\n",
    "\n",
    "\n",
    "# my_kl_loss(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.673932285662892"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "wasserstein_distance(list(range(1,101)), list(range(1,101)), list(weight_c[1,:]), list(weight_d[1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy-Based Scipy Results: \n",
      " Wasserstein distance 0.40941147635035596 \n",
      " Energy distance 0.33082122582902784 \n",
      "\n",
      "Pytorch-Based Results:\n",
      "Wasserstein loss tensor(0.4094, dtype=torch.float64) True\n",
      "Energy loss tensor(0.3308, dtype=torch.float64) True\n",
      "p == 1.5 CDF loss tensor(0.2769, dtype=torch.float64)\n",
      "Validate Checking Errors: None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#Testing Script for Statistical Loss Function\n",
    "\n",
    "BATCH = 4\n",
    "DIM = 6\n",
    "\n",
    "vec_1 = np.random.random((BATCH,DIM))\n",
    "vec_2 = np.random.random((BATCH,DIM))\n",
    "vec_list = np.arange(DIM)\n",
    "\n",
    "# Making Scipy Results\n",
    "result_1=0\n",
    "result_2=0\n",
    "for i in range(BATCH):\n",
    "    vec_dist_1 = stats.wasserstein_distance(vec_list, vec_list, vec_1[i], vec_2[i])\n",
    "    vec_dist_2 = stats.energy_distance(vec_list,vec_list,vec_1[i],vec_2[i])\n",
    "    result_1 += vec_dist_1\n",
    "    result_2 += vec_dist_2\n",
    "print(\"Numpy-Based Scipy Results: \\n\",\n",
    "      \"Wasserstein distance\",result_1/BATCH,\"\\n\",\n",
    "      \"Energy distance\",result_2/BATCH,\"\\n\")\n",
    "\n",
    "\n",
    "tensor_1=Variable(torch.from_numpy(vec_1))\n",
    "tensor_2=Variable(torch.from_numpy(vec_2),requires_grad=True)\n",
    "tensor_3=Variable(torch.rand(BATCH+1,DIM))\n",
    "\n",
    "\n",
    "#######################################################\n",
    "#       STATISTICAL DISTANCES(LOSSES) IN PYTORCH      #\n",
    "#######################################################\n",
    "\n",
    "## Statistial Distances for 1D weight distributions\n",
    "## Inspired by Scipy.Stats Statistial Distances for 1D\n",
    "## Pytorch Version, supporting Autograd to make a valid Loss\n",
    "## Supposing Inputs are Groups of Same-Length Weight Vectors\n",
    "## Instead of (Points, Weight), full-length Weight Vectors are taken as Inputs\n",
    "## Code Written by E.Bao, CASIA\n",
    "\n",
    "\n",
    "def torch_wasserstein_loss(tensor_a,tensor_b):\n",
    "    #Compute the first Wasserstein distance between two 1D distributions.\n",
    "    return(torch_cdf_loss(tensor_a,tensor_b,p=1))\n",
    "\n",
    "def torch_energy_loss(tensor_a,tensor_b):\n",
    "    # Compute the energy distance between two 1D distributions.\n",
    "    return((2**0.5)*torch_cdf_loss(tensor_a,tensor_b,p=2))\n",
    "\n",
    "def torch_cdf_loss(tensor_a,tensor_b,p=1):\n",
    "    # last-dimension is weight distribution\n",
    "    # p is the norm of the distance, p=1 --> First Wasserstein Distance\n",
    "    # to get a positive weight with our normalized distribution\n",
    "    # we recommend combining this loss with other difference-based losses like L1\n",
    "\n",
    "    # normalize distribution, add 1e-14 to divisor to avoid 0/0\n",
    "    tensor_a = tensor_a / (torch.sum(tensor_a, dim=-1, keepdim=True) + 1e-14)\n",
    "    tensor_b = tensor_b / (torch.sum(tensor_b, dim=-1, keepdim=True) + 1e-14)\n",
    "    # make cdf with cumsum\n",
    "    cdf_tensor_a = torch.cumsum(tensor_a,dim=-1)\n",
    "    cdf_tensor_b = torch.cumsum(tensor_b,dim=-1)\n",
    "\n",
    "    # choose different formulas for different norm situations\n",
    "    if p == 1:\n",
    "        cdf_distance = torch.sum(torch.abs((cdf_tensor_a-cdf_tensor_b)),dim=-1)\n",
    "    elif p == 2:\n",
    "        cdf_distance = torch.sqrt(torch.sum(torch.pow((cdf_tensor_a-cdf_tensor_b),2),dim=-1))\n",
    "    else:\n",
    "        cdf_distance = torch.pow(torch.sum(torch.pow(torch.abs(cdf_tensor_a-cdf_tensor_b),p),dim=-1),1/p)\n",
    "\n",
    "    cdf_loss = cdf_distance.mean()\n",
    "    return cdf_loss\n",
    "\n",
    "def torch_validate_distibution(tensor_a,tensor_b):\n",
    "    # Zero sized dimension is not supported by pytorch, we suppose there is no empty inputs\n",
    "    # Weights should be non-negetive, and with a positive and finite sum\n",
    "    # We suppose all conditions will be corrected by network training\n",
    "    # We only check the match of the size here\n",
    "    if tensor_a.size() != tensor_b.size():\n",
    "        raise ValueError(\"Input weight tensors must be of the same size\")\n",
    "\n",
    "print(\"Pytorch-Based Results:\")\n",
    "print(\"Wasserstein loss\",torch_wasserstein_loss(tensor_1,tensor_2).data,torch_wasserstein_loss(tensor_1,tensor_2).requires_grad)\n",
    "print(\"Energy loss\",torch_energy_loss(tensor_1,tensor_2).data,torch_wasserstein_loss(tensor_1,tensor_2).requires_grad)\n",
    "print(\"p == 1.5 CDF loss\", torch_cdf_loss(tensor_1,tensor_2,p=1.5).data)\n",
    "print(\"Validate Checking Errors:\", torch_validate_distibution(tensor_1,tensor_2))\n",
    "#torch_validate_distibution(tensor_1,tensor_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2222, 0.3333, 0.4444],\n",
      "        [0.5714, 0.2857, 0.1429]])\n",
      "tensor([0.2222, 0.3333, 0.4444])\n",
      "tensor(0.1730)\n",
      "tensor(0.1668)\n",
      "tensor(0.1699)\n",
      "tensor(0.1699)\n",
      "tensor(0.0407)\n",
      "tensor(0.0407)\n",
      "tensor(0.3254)\n",
      "tensor(0.3254)\n",
      "tensor(0.0538)\n",
      "tensor(0.0538)\n",
      "tensor(1.2338)\n",
      "tensor(1.1750)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from einops import repeat\n",
    "import torch.nn.functional as F\n",
    "torch.random.manual_seed(3)\n",
    "a = torch.tensor([[2,3,4],[4,2,1]]).float()\n",
    "a[0,:] = a[0,:] / 9\n",
    "# a[1,:] = a[0,:]\n",
    "a[1,:] = a[1,:] / 7\n",
    "b = torch.tensor([2,3,4]).float()\n",
    "b = b / b.sum()\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "def KL_loss(scores_dis, dis):\n",
    "    \"\"\"\n",
    "    Kullback Leibler Divergence\n",
    "    :param scores_dis: (B, H, L, window_size)\n",
    "    :param dis: (window_size)\n",
    "    :return: scalar\n",
    "    \"\"\"\n",
    "    kl = dis * (torch.log(dis + 0.00001) - torch.log(scores_dis + 0.00001))\n",
    "    return torch.mean(torch.sum(kl, dim=-1))\n",
    "\n",
    "\n",
    "def SKL_loss(scores_dis, dis):\n",
    "    \"\"\"\n",
    "    Symmetrized Kullback Leibler Divergence\n",
    "    :param scores_dis: (B, H, L, window_size)\n",
    "    :param dis: (window_size)\n",
    "    :return: scalar\n",
    "    \"\"\"\n",
    "    skl1 = dis * (torch.log(dis + 0.00001) - torch.log(scores_dis + 0.00001)) / 2\n",
    "    skl2 = scores_dis * (torch.log(scores_dis + 0.00001) - torch.log(dis + 0.00001)) / 2\n",
    "    return torch.mean(torch.sum(skl1 + skl2, dim=-1))\n",
    "\n",
    "\n",
    "def JS_loss(scores_dis, dis):\n",
    "    \"\"\"\n",
    "    Jensen Shannon divergence\n",
    "    :param scores_dis: (B, H, L, window_size)\n",
    "    :param dis: (window_size)\n",
    "    :return: scalar\n",
    "    \"\"\"\n",
    "    m = (scores_dis + dis) / 2\n",
    "    js1 = scores_dis * (torch.log(scores_dis + 0.00001) - torch.log(m + 0.00001)) / 2\n",
    "    js2 = dis * (torch.log(dis + 0.00001) - torch.log(m + 0.00001)) / 2\n",
    "    return torch.mean(torch.sum(js1 + js2, dim=-1))\n",
    "\n",
    "\n",
    "def W_loss(scores_dis, dis):\n",
    "    \"\"\"\n",
    "    Wasserstein distance\n",
    "    :param scores_dis: (B, H, L, window_size)\n",
    "    :param dis: (window_size)\n",
    "    :return: scalar\n",
    "    \"\"\"\n",
    "\n",
    "    # make cdf with cumsum\n",
    "    cdf_tensor_a = torch.cumsum(scores_dis, dim=-1)\n",
    "    cdf_tensor_b = torch.cumsum(dis, dim=-1)\n",
    "    return torch.mean(torch.sum(torch.abs(cdf_tensor_a-cdf_tensor_b), dim=-1))\n",
    "\n",
    "\n",
    "def L2_loss(scores_dis, dis):\n",
    "    \"\"\"\n",
    "    L2 distance (MSE)\n",
    "    :param scores_dis: (B, H, L, window_size)\n",
    "    :param dis: (window_size)\n",
    "    :return: scalar\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.mean(torch.sum((scores_dis - dis) ** 2, dim=-1)) / 2\n",
    "\n",
    "\n",
    "def CE_loss(scores_dis, dis):\n",
    "    \"\"\"\n",
    "    Cross Entropy\n",
    "    :param scores_dis: (B, H, L, window_size)\n",
    "    :param dis: (window_size)\n",
    "    :return: scalar\n",
    "    \"\"\"\n",
    "\n",
    "    return -torch.mean(torch.sum(dis * torch.log(scores_dis + 0.00001), dim=-1))\n",
    "\n",
    "print(KL_loss(a,b))\n",
    "print(KL_loss(b,a))\n",
    "print(SKL_loss(a,b))\n",
    "print(SKL_loss(b,a))\n",
    "print(JS_loss(a,b))\n",
    "print(JS_loss(b,a))\n",
    "print(W_loss(a,b))\n",
    "print(W_loss(b,a))\n",
    "print(L2_loss(a,b))\n",
    "print(L2_loss(b,a))\n",
    "print(CE_loss(a,b))\n",
    "print(CE_loss(b,a))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c3dd7e44c4c158ed28052e54f0dfbeaaf38a8338aa5f43c58a4669fde362be7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('eut')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
